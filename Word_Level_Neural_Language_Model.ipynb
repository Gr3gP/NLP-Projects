{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Level Neural Language Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOO0++I6sJkQQ/05zqbrZv1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gr3gP/NLP-Projects/blob/main/Word_Level_Neural_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycIrerJvdW3t"
      },
      "source": [
        "#Building a Word Level Neural Language Model\n",
        "\n",
        "\n",
        "\n",
        "In this paper I will be building a Neural Language Model from three of Shakespeares most famous tragedies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnHUQ1Sad1tJ"
      },
      "source": [
        "##EDA and Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcudeElWWg4N",
        "outputId": "735c388f-ff24-4046-c772-7dff9a0fb521"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!python -m spacy download en"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.1.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.7.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.1.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84wgqdK7WucS",
        "outputId": "5023b01d-066c-4ca6-9380-e6378291c554"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "import spacy\n",
        "import re\n",
        "import string\n",
        "from random import randint\n",
        "from pickle import dump, load\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, BatchNormalization, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "!python -m spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.1.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHUoXyvzXI2i"
      },
      "source": [
        "#we will use the Shakspearean tragedies. Download and concat the three into a single txt object\n",
        "\n",
        "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
        "macbeth = gutenberg.raw('shakespeare-macbeth.txt')\n",
        "caesar = gutenberg.raw('shakespeare-caesar.txt')\n",
        "\n",
        "tragedies = caesar + macbeth + hamlet"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I-DhDRYXNLy"
      },
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\treturn tokens"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ2ie2GIYXQ_",
        "outputId": "ae0c9d3f-a942-4911-86cc-492584db5ea5"
      },
      "source": [
        "#clean our document and inspect the first few hundred tokens\n",
        "tokens = clean_doc(tragedies)\n",
        "print(tokens[:200])\n",
        "print('total tokens: %d'% len(tokens))\n",
        "print('unique tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'tragedie', 'of', 'julius', 'caesar', 'by', 'william', 'shakespeare', 'actus', 'primus', 'scoena', 'prima', 'enter', 'flauius', 'murellus', 'and', 'certaine', 'commoners', 'ouer', 'the', 'stage', 'flauius', 'hence', 'home', 'you', 'idle', 'creatures', 'get', 'you', 'home', 'is', 'this', 'a', 'holiday', 'what', 'know', 'you', 'not', 'being', 'mechanicall', 'you', 'ought', 'not', 'walke', 'vpon', 'a', 'labouring', 'day', 'without', 'the', 'signe', 'of', 'your', 'profession', 'speake', 'what', 'trade', 'art', 'thou', 'car', 'why', 'sir', 'a', 'carpenter', 'mur', 'where', 'is', 'thy', 'leather', 'apron', 'and', 'thy', 'rule', 'what', 'dost', 'thou', 'with', 'thy', 'best', 'apparrell', 'on', 'you', 'sir', 'what', 'trade', 'are', 'you', 'cobl', 'truely', 'sir', 'in', 'respect', 'of', 'a', 'fine', 'workman', 'i', 'am', 'but', 'as', 'you', 'would', 'say', 'a', 'cobler', 'mur', 'but', 'what', 'trade', 'art', 'thou', 'answer', 'me', 'directly', 'cob', 'a', 'trade', 'sir', 'that', 'i', 'hope', 'i', 'may', 'vse', 'with', 'a', 'safe', 'conscience', 'which', 'is', 'indeed', 'sir', 'a', 'mender', 'of', 'bad', 'soules', 'fla', 'what', 'trade', 'thou', 'knaue', 'thou', 'naughty', 'knaue', 'what', 'trade', 'cobl', 'nay', 'i', 'beseech', 'you', 'sir', 'be', 'not', 'out', 'with', 'me', 'yet', 'if', 'you', 'be', 'out', 'sir', 'i', 'can', 'mend', 'you', 'mur', 'what', 'meanst', 'thou', 'by', 'that', 'mend', 'mee', 'thou', 'sawcy', 'fellow', 'cob', 'why', 'sir', 'cobble', 'you', 'fla', 'thou', 'art', 'a', 'cobler', 'art', 'thou', 'cob', 'truly', 'sir', 'all', 'that', 'i', 'liue', 'by', 'is']\n",
            "total tokens: 67597\n",
            "unique tokens: 7825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoicvoF4ZkmH",
        "outputId": "e0deece6-575a-4709-d681-801713285245"
      },
      "source": [
        "#organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "    #select sequence of tokens\n",
        "    seq = tokens[i - length:i]\n",
        "    #convert to a line\n",
        "    line = ' '.join(seq)\n",
        "    #store it\n",
        "    sequences.append(line)\n",
        "print('Total sequences: %d' % len(sequences))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sequences: 67546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5om4FqLcYnxH"
      },
      "source": [
        "#organize our tokens into a file\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbdqote4ZPdq"
      },
      "source": [
        "#save our sequences to a file\n",
        "out_filename = 'tragedies_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElKYyBzgaFjs"
      },
      "source": [
        "#we will now load our doc into memory\n",
        "def load_doc(filename):\n",
        "    #open file as read only\n",
        "    file = open(filename, 'r')\n",
        "    #read all text\n",
        "    text = file.read()\n",
        "    #close file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "#load\n",
        "in_filename = 'tragedies_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD9VAB71au7X"
      },
      "source": [
        "#now we can integer encode our sequences to use in our model\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "\n",
        "#set vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4AzX8F2cLhc"
      },
      "source": [
        "#seperate our model inputs and outputs\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya3VWxGgdUaC"
      },
      "source": [
        "##Build Model I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfYJ3t3seUjR"
      },
      "source": [
        "Now that we have clean and encoded our data, we are ready to begin building our model. We will use two hidden LSTM layers with 100 memory cells each to start. A fully connected Dense layers with 100 neurons will connect to the hidden LSTM layers to interpret the extracted features for the sequence. We will use a softmax activation function to ensure outputs are characteristic of normalized probabilities. We will use categorical cross entropy loss since this is technically a multi-class classification problem. An Adam implementation of mini-batch gradient descent is used as well and accuracy will be our metric. The model will run for 100 epochs and a smaller batch size of 128 to start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWkhVGZ_cnQ-",
        "outputId": "b358f14d-8688-4c77-8ea2-65c73c2c79ab"
      },
      "source": [
        "#Define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 50)            391300    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7826)              790426    \n",
            "=================================================================\n",
            "Total params: 1,332,626\n",
            "Trainable params: 1,332,626\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN_VwxxJfVl2",
        "outputId": "57ee2d28-69cb-4e09-f240-0da469ffd5b1"
      },
      "source": [
        "#compile our model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "528/528 [==============================] - 11s 15ms/step - loss: 7.2681 - accuracy: 0.0274\n",
            "Epoch 2/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 6.5555 - accuracy: 0.0360\n",
            "Epoch 3/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 6.4199 - accuracy: 0.0436\n",
            "Epoch 4/100\n",
            "528/528 [==============================] - 8s 14ms/step - loss: 6.3211 - accuracy: 0.0484\n",
            "Epoch 5/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 6.1741 - accuracy: 0.0582\n",
            "Epoch 6/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 6.0489 - accuracy: 0.0650\n",
            "Epoch 7/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.9312 - accuracy: 0.0713\n",
            "Epoch 8/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.8337 - accuracy: 0.0762\n",
            "Epoch 9/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.7356 - accuracy: 0.0809\n",
            "Epoch 10/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.6348 - accuracy: 0.0878\n",
            "Epoch 11/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.5510 - accuracy: 0.0888\n",
            "Epoch 12/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.4620 - accuracy: 0.0962\n",
            "Epoch 13/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.3863 - accuracy: 0.0955\n",
            "Epoch 14/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.3122 - accuracy: 0.0976\n",
            "Epoch 15/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.2478 - accuracy: 0.0994\n",
            "Epoch 16/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.1702 - accuracy: 0.1021\n",
            "Epoch 17/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.0774 - accuracy: 0.1080\n",
            "Epoch 18/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.0063 - accuracy: 0.1065\n",
            "Epoch 19/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.9406 - accuracy: 0.1082\n",
            "Epoch 20/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.8501 - accuracy: 0.1153\n",
            "Epoch 21/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.7915 - accuracy: 0.1171\n",
            "Epoch 22/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.7399 - accuracy: 0.1189\n",
            "Epoch 23/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.6644 - accuracy: 0.1217\n",
            "Epoch 24/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.5884 - accuracy: 0.1303\n",
            "Epoch 25/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.5409 - accuracy: 0.1318\n",
            "Epoch 26/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.4670 - accuracy: 0.1408\n",
            "Epoch 27/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.4352 - accuracy: 0.1403\n",
            "Epoch 28/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.3737 - accuracy: 0.1513\n",
            "Epoch 29/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.3257 - accuracy: 0.1522\n",
            "Epoch 30/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.2795 - accuracy: 0.1609\n",
            "Epoch 31/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.2250 - accuracy: 0.1631\n",
            "Epoch 32/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.1875 - accuracy: 0.1713\n",
            "Epoch 33/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.1393 - accuracy: 0.1769\n",
            "Epoch 34/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.0982 - accuracy: 0.1870\n",
            "Epoch 35/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.0549 - accuracy: 0.1881\n",
            "Epoch 36/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.0343 - accuracy: 0.1911\n",
            "Epoch 37/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.9947 - accuracy: 0.1977\n",
            "Epoch 38/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.9482 - accuracy: 0.2025\n",
            "Epoch 39/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.9044 - accuracy: 0.2106\n",
            "Epoch 40/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.8912 - accuracy: 0.2099\n",
            "Epoch 41/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.8482 - accuracy: 0.2188\n",
            "Epoch 42/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.8172 - accuracy: 0.2219\n",
            "Epoch 43/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.7883 - accuracy: 0.2255\n",
            "Epoch 44/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.7686 - accuracy: 0.2283\n",
            "Epoch 45/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.7220 - accuracy: 0.2379\n",
            "Epoch 46/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.7015 - accuracy: 0.2361\n",
            "Epoch 47/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.6836 - accuracy: 0.2411\n",
            "Epoch 48/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.6336 - accuracy: 0.2469\n",
            "Epoch 49/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.6123 - accuracy: 0.2513\n",
            "Epoch 50/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.5838 - accuracy: 0.2548\n",
            "Epoch 51/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.5591 - accuracy: 0.2602\n",
            "Epoch 52/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.5310 - accuracy: 0.2642\n",
            "Epoch 53/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.4888 - accuracy: 0.2691\n",
            "Epoch 54/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.4753 - accuracy: 0.2734\n",
            "Epoch 55/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.4444 - accuracy: 0.2763\n",
            "Epoch 56/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.4100 - accuracy: 0.2807\n",
            "Epoch 57/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.3964 - accuracy: 0.2839\n",
            "Epoch 58/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.3773 - accuracy: 0.2873\n",
            "Epoch 59/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.3454 - accuracy: 0.2953\n",
            "Epoch 60/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.3421 - accuracy: 0.2954\n",
            "Epoch 61/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.3058 - accuracy: 0.3006\n",
            "Epoch 62/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.2811 - accuracy: 0.3023\n",
            "Epoch 63/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.2696 - accuracy: 0.3074\n",
            "Epoch 64/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.2385 - accuracy: 0.3135\n",
            "Epoch 65/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.2245 - accuracy: 0.3140\n",
            "Epoch 66/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.1918 - accuracy: 0.3198\n",
            "Epoch 67/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.1843 - accuracy: 0.3192\n",
            "Epoch 68/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.1605 - accuracy: 0.3215\n",
            "Epoch 69/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.1346 - accuracy: 0.3287\n",
            "Epoch 70/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.1106 - accuracy: 0.3325\n",
            "Epoch 71/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.0943 - accuracy: 0.3344\n",
            "Epoch 72/100\n",
            "528/528 [==============================] - 8s 14ms/step - loss: 3.0679 - accuracy: 0.3409\n",
            "Epoch 73/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.0645 - accuracy: 0.3409\n",
            "Epoch 74/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.0326 - accuracy: 0.3485\n",
            "Epoch 75/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.0141 - accuracy: 0.3494\n",
            "Epoch 76/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.0034 - accuracy: 0.3495\n",
            "Epoch 77/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.9820 - accuracy: 0.3567\n",
            "Epoch 78/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.9758 - accuracy: 0.3572\n",
            "Epoch 79/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.9540 - accuracy: 0.3579\n",
            "Epoch 80/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.9363 - accuracy: 0.3614\n",
            "Epoch 81/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.9095 - accuracy: 0.3669\n",
            "Epoch 82/100\n",
            "528/528 [==============================] - 8s 14ms/step - loss: 2.9005 - accuracy: 0.3694\n",
            "Epoch 83/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.8867 - accuracy: 0.3719\n",
            "Epoch 84/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.8630 - accuracy: 0.3770\n",
            "Epoch 85/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.8532 - accuracy: 0.3769\n",
            "Epoch 86/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.8354 - accuracy: 0.3819\n",
            "Epoch 87/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.8291 - accuracy: 0.3801\n",
            "Epoch 88/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.7964 - accuracy: 0.3857\n",
            "Epoch 89/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.7910 - accuracy: 0.3888\n",
            "Epoch 90/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.7678 - accuracy: 0.3928\n",
            "Epoch 91/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.7513 - accuracy: 0.3951\n",
            "Epoch 92/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.7505 - accuracy: 0.3952\n",
            "Epoch 93/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.7161 - accuracy: 0.4018\n",
            "Epoch 94/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.7022 - accuracy: 0.3999\n",
            "Epoch 95/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.6863 - accuracy: 0.4080\n",
            "Epoch 96/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.6798 - accuracy: 0.4063\n",
            "Epoch 97/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.6613 - accuracy: 0.4130\n",
            "Epoch 98/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.6487 - accuracy: 0.4133\n",
            "Epoch 99/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.6263 - accuracy: 0.4202\n",
            "Epoch 100/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.6034 - accuracy: 0.4196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0103661190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMVeY_RGg_Vo",
        "outputId": "8ed75675-578b-46a0-d822-b621c6a690ec"
      },
      "source": [
        "#save model to file\n",
        "model.save('model.g1')\n",
        "#save the tokenizer \n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.g1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.g1/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoByKAIkkpAS"
      },
      "source": [
        "##Load the Data and Model I"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyNU8lfViWSU"
      },
      "source": [
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "    #open file\n",
        "    file = open(filename, 'r')\n",
        "    #read text\n",
        "    text = file.read()\n",
        "    #close file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "#load cleaned text sequences\n",
        "in_filename = 'tragedies_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "#change sequence length\n",
        "seq_length = len(lines[0].split()) - 1\n",
        "\n",
        "#load model\n",
        "model = load_model('model.g1')\n",
        "\n",
        "#load tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHvJ4sBpkuJU"
      },
      "source": [
        "##Generate Text with Model I"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjELDpqckbTG",
        "outputId": "8f742c7c-ffd0-4d8c-dd32-c9d7c71a28e6"
      },
      "source": [
        "#select a random seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "queene would speak with you and presently ham do you see that clowd thats almost in shape like a camell polon byth masse and its like a camell indeed ham me thinkes it is like a weazell polon it is backd like a weazell ham or like a whale polon verie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eh4YQ5dmYCX"
      },
      "source": [
        "#generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "    #generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        #encode text as integers\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        #truncate the sequence\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        #predict probabilites for each word\n",
        "        yhat = model.predict_classes(encoded, verbose=0)\n",
        "        #map predicted word index to words\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        #append to our input\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBcb4tJbnnqc",
        "outputId": "4ed5dd1d-e6b9-427e-e338-aa7dc93bd660"
      },
      "source": [
        "#lets generate some text\n",
        "generated = generate_seq(model, tokenizer, seq_length,seed_text, 50)\n",
        "print(generated)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "friends ophe i am glad to see you by your health and shalt not presume but to me indifferent oath to signifie it i am i shall be bethinke me i had not quoted him i will not thinke and i shall finde them crownd in the pit of tyber\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37fdKMPj5I6t"
      },
      "source": [
        "We can see the model ran for 100 epochs and gave us fairly unitelligible text. We can try adding in dropout or batch normalization to improve its performance as well as running it for more epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVKttxenMzvs"
      },
      "source": [
        "##Build Model II"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZitSGO5h-zQA",
        "outputId": "2726dca7-017c-4767-a1b7-2f9147a94539"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 50)            391300    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 7826)              790426    \n",
            "=================================================================\n",
            "Total params: 1,333,026\n",
            "Trainable params: 1,332,826\n",
            "Non-trainable params: 200\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD7OlhKlANiL",
        "outputId": "c33b594c-0f8b-46a1-b118-685a220e6d3b"
      },
      "source": [
        "#compile new model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "528/528 [==============================] - 10s 15ms/step - loss: 7.2773 - accuracy: 0.0288\n",
            "Epoch 2/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 6.5988 - accuracy: 0.0366\n",
            "Epoch 3/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 6.4407 - accuracy: 0.0447\n",
            "Epoch 4/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 6.3380 - accuracy: 0.0468\n",
            "Epoch 5/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 6.2588 - accuracy: 0.0464\n",
            "Epoch 6/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 6.2416 - accuracy: 0.0480\n",
            "Epoch 7/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 6.1206 - accuracy: 0.0533\n",
            "Epoch 8/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 6.0378 - accuracy: 0.0591\n",
            "Epoch 9/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.9674 - accuracy: 0.0632\n",
            "Epoch 10/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.9046 - accuracy: 0.0649\n",
            "Epoch 11/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.8458 - accuracy: 0.0693\n",
            "Epoch 12/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.7770 - accuracy: 0.0749\n",
            "Epoch 13/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.7112 - accuracy: 0.0791\n",
            "Epoch 14/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.6518 - accuracy: 0.0838\n",
            "Epoch 15/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.5863 - accuracy: 0.0878\n",
            "Epoch 16/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.6776 - accuracy: 0.0817\n",
            "Epoch 17/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.5098 - accuracy: 0.0918\n",
            "Epoch 18/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.4327 - accuracy: 0.0945\n",
            "Epoch 19/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.3487 - accuracy: 0.1008\n",
            "Epoch 20/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.2797 - accuracy: 0.1047\n",
            "Epoch 21/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.1937 - accuracy: 0.1095\n",
            "Epoch 22/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.1251 - accuracy: 0.1143\n",
            "Epoch 23/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 5.0664 - accuracy: 0.1165\n",
            "Epoch 24/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.9904 - accuracy: 0.1211\n",
            "Epoch 25/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.9270 - accuracy: 0.1282\n",
            "Epoch 26/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.8484 - accuracy: 0.1306\n",
            "Epoch 27/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.7987 - accuracy: 0.1338\n",
            "Epoch 28/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.7474 - accuracy: 0.1355\n",
            "Epoch 29/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.6677 - accuracy: 0.1445\n",
            "Epoch 30/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.6222 - accuracy: 0.1461\n",
            "Epoch 31/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.5613 - accuracy: 0.1501\n",
            "Epoch 32/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.4966 - accuracy: 0.1530\n",
            "Epoch 33/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.4490 - accuracy: 0.1568\n",
            "Epoch 34/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.4061 - accuracy: 0.1613\n",
            "Epoch 35/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.3387 - accuracy: 0.1649\n",
            "Epoch 36/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.3085 - accuracy: 0.1688\n",
            "Epoch 37/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.2711 - accuracy: 0.1722\n",
            "Epoch 38/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.2096 - accuracy: 0.1748\n",
            "Epoch 39/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.1675 - accuracy: 0.1781\n",
            "Epoch 40/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.1182 - accuracy: 0.1850\n",
            "Epoch 41/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.0775 - accuracy: 0.1918\n",
            "Epoch 42/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.0521 - accuracy: 0.1936\n",
            "Epoch 43/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 4.0018 - accuracy: 0.1984\n",
            "Epoch 44/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.9881 - accuracy: 0.1979\n",
            "Epoch 45/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.9681 - accuracy: 0.2016\n",
            "Epoch 46/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.9120 - accuracy: 0.2061\n",
            "Epoch 47/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.8828 - accuracy: 0.2107\n",
            "Epoch 48/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.8484 - accuracy: 0.2127\n",
            "Epoch 49/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.8143 - accuracy: 0.2181\n",
            "Epoch 50/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.7863 - accuracy: 0.2220\n",
            "Epoch 51/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.7513 - accuracy: 0.2290\n",
            "Epoch 52/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.7172 - accuracy: 0.2335\n",
            "Epoch 53/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.6990 - accuracy: 0.2335\n",
            "Epoch 54/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.6764 - accuracy: 0.2367\n",
            "Epoch 55/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.6482 - accuracy: 0.2382\n",
            "Epoch 56/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.6349 - accuracy: 0.2419\n",
            "Epoch 57/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.5841 - accuracy: 0.2477\n",
            "Epoch 58/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.5703 - accuracy: 0.2505\n",
            "Epoch 59/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.5396 - accuracy: 0.2545\n",
            "Epoch 60/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.5200 - accuracy: 0.2566\n",
            "Epoch 61/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.5009 - accuracy: 0.2603\n",
            "Epoch 62/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.4743 - accuracy: 0.2637\n",
            "Epoch 63/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.4563 - accuracy: 0.2631\n",
            "Epoch 64/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.4277 - accuracy: 0.2694\n",
            "Epoch 65/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.3856 - accuracy: 0.2761\n",
            "Epoch 66/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.3754 - accuracy: 0.2744\n",
            "Epoch 67/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.3497 - accuracy: 0.2822\n",
            "Epoch 68/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.3409 - accuracy: 0.2773\n",
            "Epoch 69/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.3187 - accuracy: 0.2823\n",
            "Epoch 70/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.2920 - accuracy: 0.2869\n",
            "Epoch 71/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.2645 - accuracy: 0.2944\n",
            "Epoch 72/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.2582 - accuracy: 0.2955\n",
            "Epoch 73/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.2240 - accuracy: 0.2981\n",
            "Epoch 74/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.2023 - accuracy: 0.3026\n",
            "Epoch 75/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.1967 - accuracy: 0.3055\n",
            "Epoch 76/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.1590 - accuracy: 0.3102\n",
            "Epoch 77/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.1547 - accuracy: 0.3074\n",
            "Epoch 78/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.1248 - accuracy: 0.3192\n",
            "Epoch 79/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.1055 - accuracy: 0.3183\n",
            "Epoch 80/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.1158 - accuracy: 0.3166\n",
            "Epoch 81/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.0741 - accuracy: 0.3206\n",
            "Epoch 82/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.0733 - accuracy: 0.3209\n",
            "Epoch 83/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.0571 - accuracy: 0.3264\n",
            "Epoch 84/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.0242 - accuracy: 0.3272\n",
            "Epoch 85/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 3.0200 - accuracy: 0.3300\n",
            "Epoch 86/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.9833 - accuracy: 0.3355\n",
            "Epoch 87/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.9805 - accuracy: 0.3376\n",
            "Epoch 88/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.9527 - accuracy: 0.3430\n",
            "Epoch 89/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.9265 - accuracy: 0.3458\n",
            "Epoch 90/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.9276 - accuracy: 0.3451\n",
            "Epoch 91/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.9097 - accuracy: 0.3477\n",
            "Epoch 92/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.8948 - accuracy: 0.3487\n",
            "Epoch 93/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.8786 - accuracy: 0.3520\n",
            "Epoch 94/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.8646 - accuracy: 0.3559\n",
            "Epoch 95/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.8421 - accuracy: 0.3603\n",
            "Epoch 96/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.8429 - accuracy: 0.3569\n",
            "Epoch 97/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.8125 - accuracy: 0.3662\n",
            "Epoch 98/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.7990 - accuracy: 0.3658\n",
            "Epoch 99/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.7901 - accuracy: 0.3654\n",
            "Epoch 100/100\n",
            "528/528 [==============================] - 8s 15ms/step - loss: 2.7756 - accuracy: 0.3707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f004e6d4a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBAwXQOjAuvO",
        "outputId": "0c82d9ed-f4b3-476a-e4d0-515aa3712484"
      },
      "source": [
        "#Save model\n",
        "model.save('model.g2')\n",
        "#save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.g2/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model.g2/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV6FWbZOApEW"
      },
      "source": [
        "##Load Data and Model II"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA47KXvhAjO3"
      },
      "source": [
        "#load dco into memory\n",
        "def load_doc(filename):\n",
        "    #open file \n",
        "    file = open(filename, 'r')\n",
        "    #read text\n",
        "    text = file.read()\n",
        "    #close file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "#Load cleaned text\n",
        "in_filename = 'tragedies_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "#change sequence length\n",
        "seq_length = len(lines[0].split()) -1\n",
        "\n",
        "#load model\n",
        "model = load_model('model.g2')\n",
        "\n",
        "#load tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va1v43QgGzVN"
      },
      "source": [
        "##Generate Text with Model II"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wf6U3G7G2HR",
        "outputId": "0f159280-63c8-464a-bb64-379f1cde08f7"
      },
      "source": [
        "#Start with a random seed text\n",
        "seed_text = lines[randint(0, len(lines))]\n",
        "print(seed_text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tis very hot ham no beleeue mee tis very cold the winde is northerly osr it is indifferent cold my lord indeed ham mee thinkes it is very soultry and hot for my complexion osr exceedingly my lord it is very soultry as twere i cannot tell how but my lord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5KJ2AxqGu4I",
        "outputId": "e275382e-9b77-4bfc-93ce-d2eba24ff721"
      },
      "source": [
        "#call function to generate 10 words \n",
        "text_generate10 = generate_seq(model, tokenizer, seq_length, seed_text, 10)\n",
        "print(text_generate10)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham eene as i slewe my sad poysond bru and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siM8BPVvLyNm"
      },
      "source": [
        "We can see that our model wasn't half bad at producing semi-intelligible text this time around. It is also worth noting that as our word count increases, our sentence coherence decreases with the model just spewing words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0vHpO-bHMQD"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}